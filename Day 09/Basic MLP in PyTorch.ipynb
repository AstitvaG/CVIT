{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Base",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Basic MLP in PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO152_NakUnH",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "### Data\n",
        "\n",
        "**MNIST Data**: Modified National Institute of Standards and Technology database\n",
        "\n",
        "Digit representation in pixels, a large database of handwritten digits that is commonly used for training various image processing systems.\n",
        "\n",
        "The MNIST database contains 60,000 training images and 10,000 testing images taken from American Census Bureau employees and American high school students."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "120vlXJQf6Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEI4qvVGf6F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwNRRDgWf6GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp8wY5W1f6GM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8UUUraJf6GV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjaGif13f6Ge",
        "colab_type": "text"
      },
      "source": [
        "In PyTorch we usually process multiple samples at the same time. Usually we stack up each training vector along rows to create $X$ matrix.\n",
        "\n",
        "\\begin{equation}\n",
        "X = \\begin{pmatrix}\n",
        "- & - & x_1 & - & -\\\\\n",
        "- & - & x_2 & - & -\\\\\n",
        ". & . & . & . & .\\\\\n",
        ". & . & . & . & .\\\\\n",
        ". & . & . & . & .\\\\\n",
        ". & . & . & . & .\\\\\n",
        "- & - & x_{batch-size} & - & -\\\\\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "torch.nn.Linear class when instantiated creates an object with a randomly initialised $W$ matrix of dimensions(in_class features, out_class features) and another randomly initialised matrix $b$ of dimensions(out_class features, 1). These dimensions are passed as arguments to the constructor during creation of object. When the forward function of the created object is invoked with parameters $X$, it applies linear transformation $XW + b$ and returns the appropriate matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwuKUEJaf6Gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a Neural Network\n",
        "\n",
        "# Inherits from the NN module\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Construct objects of the NN module class\n",
        "        # Input layer with 784 inputs and 128 outputs \n",
        "        self.fc1 = torch.nn.Linear(784, 128)\n",
        "        # Hidden layer with 128 inputs and 64 outputs \n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        # Output layer with 64 inputs and 10 outputs \n",
        "        self.fc3 = torch.nn.Linear(64, 10)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        # Softmax normalizes over the columns \n",
        "        self.softmax = torch.nn.Softmax(dim = 1)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        # x has dimensions of 64, 1, 28, 28\n",
        "        # Reshape the matrix to get 64, 784\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1.forward(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAf5NNtWf6Gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an object of class MLP\n",
        "model = MLP()\n",
        "\n",
        "# Define the loss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFSAd1wsf6Gu",
        "colab_type": "code",
        "outputId": "d6ee59e9-1456-4ba7-b45e-30eb2b2578b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 100\n",
        "for e in range(epochs):\n",
        "    print(\"Epoch \", (e + 1))\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model.forward(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in testloader:\n",
        "                logit = model(images)\n",
        "                ps = logit\n",
        "                top_p, top_class = ps.topk(1, dim = 1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        print(\"Loss on training set: \", running_loss)\n",
        "        print(\"Accuracy on test set: \", end = \" \")\n",
        "        print((accuracy / len(testloader)).data.numpy())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1\n",
            "Loss on training set:  1842.2246737480164\n",
            "Accuracy on test set:  0.71616244\n",
            "Epoch  2\n",
            "Loss on training set:  1532.93612074852\n",
            "Accuracy on test set:  0.89938295\n",
            "Epoch  3\n",
            "Loss on training set:  1475.395410656929\n",
            "Accuracy on test set:  0.9111266\n",
            "Epoch  4\n",
            "Loss on training set:  1462.8611141443253\n",
            "Accuracy on test set:  0.9138137\n",
            "Epoch  5\n",
            "Loss on training set:  1454.5161266326904\n",
            "Accuracy on test set:  0.9193869\n",
            "Epoch  6\n",
            "Loss on training set:  1448.6384273767471\n",
            "Accuracy on test set:  0.92157644\n",
            "Epoch  7\n",
            "Loss on training set:  1443.8615982532501\n",
            "Accuracy on test set:  0.9263535\n",
            "Epoch  8\n",
            "Loss on training set:  1439.0422087907791\n",
            "Accuracy on test set:  0.93162817\n",
            "Epoch  9\n",
            "Loss on training set:  1435.063784480095\n",
            "Accuracy on test set:  0.9369029\n",
            "Epoch  10\n",
            "Loss on training set:  1431.1449321508408\n",
            "Accuracy on test set:  0.9403862\n",
            "Epoch  11\n",
            "Loss on training set:  1427.2730686664581\n",
            "Accuracy on test set:  0.9426752\n",
            "Epoch  12\n",
            "Loss on training set:  1424.5146894454956\n",
            "Accuracy on test set:  0.9430733\n",
            "Epoch  13\n",
            "Loss on training set:  1421.3832541704178\n",
            "Accuracy on test set:  0.94526273\n",
            "Epoch  14\n",
            "Loss on training set:  1418.8431572914124\n",
            "Accuracy on test set:  0.9426752\n",
            "Epoch  15\n",
            "Loss on training set:  1416.5349078178406\n",
            "Accuracy on test set:  0.9504379\n",
            "Epoch  16\n",
            "Loss on training set:  1414.3873839378357\n",
            "Accuracy on test set:  0.95242834\n",
            "Epoch  17\n",
            "Loss on training set:  1412.6279428005219\n",
            "Accuracy on test set:  0.9560112\n",
            "Epoch  18\n",
            "Loss on training set:  1410.5258753299713\n",
            "Accuracy on test set:  0.9558121\n",
            "Epoch  19\n",
            "Loss on training set:  1409.3464052677155\n",
            "Accuracy on test set:  0.95431924\n",
            "Epoch  20\n",
            "Loss on training set:  1407.6693338155746\n",
            "Accuracy on test set:  0.9570064\n",
            "Epoch  21\n",
            "Loss on training set:  1406.1495480537415\n",
            "Accuracy on test set:  0.9571059\n",
            "Epoch  22\n",
            "Loss on training set:  1405.0011229515076\n",
            "Accuracy on test set:  0.96267915\n",
            "Epoch  23\n",
            "Loss on training set:  1403.7116985321045\n",
            "Accuracy on test set:  0.9633758\n",
            "Epoch  24\n",
            "Loss on training set:  1402.6855894327164\n",
            "Accuracy on test set:  0.96367437\n",
            "Epoch  25\n",
            "Loss on training set:  1401.2603820562363\n",
            "Accuracy on test set:  0.9616839\n",
            "Epoch  26\n",
            "Loss on training set:  1400.167906999588\n",
            "Accuracy on test set:  0.9638734\n",
            "Epoch  27\n",
            "Loss on training set:  1399.1515941619873\n",
            "Accuracy on test set:  0.96715766\n",
            "Epoch  28\n",
            "Loss on training set:  1398.4862405061722\n",
            "Accuracy on test set:  0.9658638\n",
            "Epoch  29\n",
            "Loss on training set:  1397.3338918685913\n",
            "Accuracy on test set:  0.96755576\n",
            "Epoch  30\n",
            "Loss on training set:  1396.5371675491333\n",
            "Accuracy on test set:  0.9679538\n",
            "Epoch  31\n",
            "Loss on training set:  1395.7792712450027\n",
            "Accuracy on test set:  0.9656648\n",
            "Epoch  32\n",
            "Loss on training set:  1394.870719075203\n",
            "Accuracy on test set:  0.9683519\n",
            "Epoch  33\n",
            "Loss on training set:  1394.0141788721085\n",
            "Accuracy on test set:  0.9648686\n",
            "Epoch  34\n",
            "Loss on training set:  1393.1148365736008\n",
            "Accuracy on test set:  0.96904856\n",
            "Epoch  35\n",
            "Loss on training set:  1392.6487411260605\n",
            "Accuracy on test set:  0.97034234\n",
            "Epoch  36\n",
            "Loss on training set:  1392.039314031601\n",
            "Accuracy on test set:  0.96924764\n",
            "Epoch  37\n",
            "Loss on training set:  1391.1184862852097\n",
            "Accuracy on test set:  0.9709395\n",
            "Epoch  38\n",
            "Loss on training set:  1390.7911332845688\n",
            "Accuracy on test set:  0.9691481\n",
            "Epoch  39\n",
            "Loss on training set:  1390.456597328186\n",
            "Accuracy on test set:  0.96994424\n",
            "Epoch  40\n",
            "Loss on training set:  1389.7329404354095\n",
            "Accuracy on test set:  0.971039\n",
            "Epoch  41\n",
            "Loss on training set:  1389.2684363126755\n",
            "Accuracy on test set:  0.97034234\n",
            "Epoch  42\n",
            "Loss on training set:  1388.739998459816\n",
            "Accuracy on test set:  0.9704419\n",
            "Epoch  43\n",
            "Loss on training set:  1388.2777663469315\n",
            "Accuracy on test set:  0.96865046\n",
            "Epoch  44\n",
            "Loss on training set:  1388.226881146431\n",
            "Accuracy on test set:  0.9714371\n",
            "Epoch  45\n",
            "Loss on training set:  1387.845923781395\n",
            "Accuracy on test set:  0.9723328\n",
            "Epoch  46\n",
            "Loss on training set:  1387.569459080696\n",
            "Accuracy on test set:  0.9712381\n",
            "Epoch  47\n",
            "Loss on training set:  1387.0388041734695\n",
            "Accuracy on test set:  0.96666\n",
            "Epoch  48\n",
            "Loss on training set:  1386.8340020179749\n",
            "Accuracy on test set:  0.97292995\n",
            "Epoch  49\n",
            "Loss on training set:  1386.4774844646454\n",
            "Accuracy on test set:  0.9741242\n",
            "Epoch  50\n",
            "Loss on training set:  1386.2518286705017\n",
            "Accuracy on test set:  0.973129\n",
            "Epoch  51\n",
            "Loss on training set:  1385.9138400554657\n",
            "Accuracy on test set:  0.97213376\n",
            "Epoch  52\n",
            "Loss on training set:  1385.4613741636276\n",
            "Accuracy on test set:  0.97253186\n",
            "Epoch  53\n",
            "Loss on training set:  1385.1844593286514\n",
            "Accuracy on test set:  0.9723328\n",
            "Epoch  54\n",
            "Loss on training set:  1384.8224534988403\n",
            "Accuracy on test set:  0.973129\n",
            "Epoch  55\n",
            "Loss on training set:  1384.6808453798294\n",
            "Accuracy on test set:  0.9732285\n",
            "Epoch  56\n",
            "Loss on training set:  1384.499952197075\n",
            "Accuracy on test set:  0.9732285\n",
            "Epoch  57\n",
            "Loss on training set:  1384.1376266479492\n",
            "Accuracy on test set:  0.9727309\n",
            "Epoch  58\n",
            "Loss on training set:  1383.9896450042725\n",
            "Accuracy on test set:  0.97292995\n",
            "Epoch  59\n",
            "Loss on training set:  1383.8460632562637\n",
            "Accuracy on test set:  0.9720342\n",
            "Epoch  60\n",
            "Loss on training set:  1383.5803978443146\n",
            "Accuracy on test set:  0.9709395\n",
            "Epoch  61\n",
            "Loss on training set:  1383.2837829589844\n",
            "Accuracy on test set:  0.97382563\n",
            "Epoch  62\n",
            "Loss on training set:  1383.2097135782242\n",
            "Accuracy on test set:  0.97402465\n",
            "Epoch  63\n",
            "Loss on training set:  1383.052651643753\n",
            "Accuracy on test set:  0.9745223\n",
            "Epoch  64\n",
            "Loss on training set:  1382.9957268238068\n",
            "Accuracy on test set:  0.97292995\n",
            "Epoch  65\n",
            "Loss on training set:  1382.7326917648315\n",
            "Accuracy on test set:  0.97382563\n",
            "Epoch  66\n",
            "Loss on training set:  1382.596786737442\n",
            "Accuracy on test set:  0.97382563\n",
            "Epoch  67\n",
            "Loss on training set:  1382.3965446949005\n",
            "Accuracy on test set:  0.968551\n",
            "Epoch  68\n",
            "Loss on training set:  1382.286662220955\n",
            "Accuracy on test set:  0.9732285\n",
            "Epoch  69\n",
            "Loss on training set:  1382.1781967878342\n",
            "Accuracy on test set:  0.9736266\n",
            "Epoch  70\n",
            "Loss on training set:  1382.1320445537567\n",
            "Accuracy on test set:  0.9736266\n",
            "Epoch  71\n",
            "Loss on training set:  1381.7460827827454\n",
            "Accuracy on test set:  0.97302943\n",
            "Epoch  72\n",
            "Loss on training set:  1381.6893017292023\n",
            "Accuracy on test set:  0.97402465\n",
            "Epoch  73\n",
            "Loss on training set:  1381.5117378234863\n",
            "Accuracy on test set:  0.97442275\n",
            "Epoch  74\n",
            "Loss on training set:  1381.338051557541\n",
            "Accuracy on test set:  0.9735271\n",
            "Epoch  75\n",
            "Loss on training set:  1381.1933554410934\n",
            "Accuracy on test set:  0.9745223\n",
            "Epoch  76\n",
            "Loss on training set:  1381.1253142356873\n",
            "Accuracy on test set:  0.97292995\n",
            "Epoch  77\n",
            "Loss on training set:  1380.9373717308044\n",
            "Accuracy on test set:  0.97342753\n",
            "Epoch  78\n",
            "Loss on training set:  1380.7706806659698\n",
            "Accuracy on test set:  0.9749204\n",
            "Epoch  79\n",
            "Loss on training set:  1380.6267191171646\n",
            "Accuracy on test set:  0.97402465\n",
            "Epoch  80\n",
            "Loss on training set:  1380.668596982956\n",
            "Accuracy on test set:  0.97442275\n",
            "Epoch  81\n",
            "Loss on training set:  1380.5442214012146\n",
            "Accuracy on test set:  0.9745223\n",
            "Epoch  82\n",
            "Loss on training set:  1380.404748558998\n",
            "Accuracy on test set:  0.97482085\n",
            "Epoch  83\n",
            "Loss on training set:  1380.3327802419662\n",
            "Accuracy on test set:  0.97442275\n",
            "Epoch  84\n",
            "Loss on training set:  1380.2416112422943\n",
            "Accuracy on test set:  0.97462183\n",
            "Epoch  85\n",
            "Loss on training set:  1380.2168797254562\n",
            "Accuracy on test set:  0.9743233\n",
            "Epoch  86\n",
            "Loss on training set:  1380.1414676904678\n",
            "Accuracy on test set:  0.9749204\n",
            "Epoch  87\n",
            "Loss on training set:  1380.1181226968765\n",
            "Accuracy on test set:  0.97422373\n",
            "Epoch  88\n",
            "Loss on training set:  1380.0809469223022\n",
            "Accuracy on test set:  0.97521895\n",
            "Epoch  89\n",
            "Loss on training set:  1380.07657456398\n",
            "Accuracy on test set:  0.9747213\n",
            "Epoch  90\n",
            "Loss on training set:  1379.9689801931381\n",
            "Accuracy on test set:  0.9741242\n",
            "Epoch  91\n",
            "Loss on training set:  1379.8679909706116\n",
            "Accuracy on test set:  0.9743233\n",
            "Epoch  92\n",
            "Loss on training set:  1379.875093460083\n",
            "Accuracy on test set:  0.9745223\n",
            "Epoch  93\n",
            "Loss on training set:  1379.8063567876816\n",
            "Accuracy on test set:  0.97442275\n",
            "Epoch  94\n",
            "Loss on training set:  1379.797855257988\n",
            "Accuracy on test set:  0.97402465\n",
            "Epoch  95\n",
            "Loss on training set:  1379.6676921844482\n",
            "Accuracy on test set:  0.97382563\n",
            "Epoch  96\n",
            "Loss on training set:  1379.6174359321594\n",
            "Accuracy on test set:  0.9739252\n",
            "Epoch  97\n",
            "Loss on training set:  1379.5659928321838\n",
            "Accuracy on test set:  0.97422373\n",
            "Epoch  98\n",
            "Loss on training set:  1379.562816977501\n",
            "Accuracy on test set:  0.97462183\n",
            "Epoch  99\n",
            "Loss on training set:  1379.5014789104462\n",
            "Accuracy on test set:  0.9753185\n",
            "Epoch  100\n",
            "Loss on training set:  1379.5385893583298\n",
            "Accuracy on test set:  0.9745223\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}