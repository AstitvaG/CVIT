{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcvdfnyDn35H",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFYSpu2Rn35J",
        "colab_type": "text"
      },
      "source": [
        "### Padding Summary\n",
        "- **Valid** Padding (Zero Padding)\n",
        "    - Output size < Input Size\n",
        "- **Same** Padding\n",
        "    - Output size = Input Size\n",
        "\n",
        "### Dimension Calculations\n",
        "- $ O = \\frac {W - K + 2P}{S} + 1$\n",
        "    - $O$: output height/length\n",
        "    - $W$: input height/length\n",
        "    - $K$: filter size (kernel size)\n",
        "    - $P$: padding\n",
        "        - $ P = \\frac{K - 1}{2} $\n",
        "    - $S$: stride\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2TbousE3n35M",
        "colab_type": "text"
      },
      "source": [
        "### Building a Convolutional Neural Network with PyTorch\n",
        "\n",
        "### Model A: \n",
        "- 2 Convolutional Layers\n",
        "    - Same Padding (same output size)\n",
        "- 2 Max Pooling Layers\n",
        "- 1 Fully Connected Layer\n",
        "\n",
        "### Steps\n",
        "- Step 1: Load Dataset\n",
        "- Step 2: Make Dataset Iterable\n",
        "- Step 3: Create Model Class\n",
        "- Step 4: Instantiate Model Class\n",
        "- Step 5: Instantiate Loss Class\n",
        "- Step 6: Instantiate Optimizer Class\n",
        "- Step 7: Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad33GaEQn35P",
        "colab_type": "text"
      },
      "source": [
        "### Loading MNIST Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAuHmkHhn35S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEu4ilqnn35j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRZ1F_Isn35s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5cbeb2dc-906e-4c28-db30-2d47ee989120"
      },
      "source": [
        "print(train_dataset.train_data.size())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKV51eeAn350",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d4ca5f13-7e9f-4e6c-9c07-98481cd90c05"
      },
      "source": [
        "print(train_dataset.train_labels.size())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTJkyldNn358",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f25a4992-8a5a-499c-d77e-dda546306dd6"
      },
      "source": [
        "print(test_dataset.test_data.size())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfaQQOqXn36F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ec6b9cb1-39bd-468a-a2d6-768ef514f79f"
      },
      "source": [
        "print(test_dataset.test_labels.size())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iabIpH9Kn36M",
        "colab_type": "text"
      },
      "source": [
        "### Make Dataset Iterable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_iO3dInn36O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTSA_ZUNn36U",
        "colab_type": "text"
      },
      "source": [
        "### Create Model Class\n",
        "\n",
        "**Output Formula for Convolution**\n",
        "- $ O = \\frac {W - K + 2P}{S} + 1$\n",
        "    - $O$: output height/length\n",
        "    - $W$: input height/length\n",
        "    - $K$: **filter size (kernel size) = 5**\n",
        "    - $P$: **same padding (non-zero)**\n",
        "        - $P = \\frac{K - 1}{2}  = \\frac{5 - 1}{2} = 2$\n",
        "    - $S$: **stride = 1**\n",
        "    \n",
        "**Output Formula for Pooling**\n",
        "- $ O = \\frac {W - K}{S} + 1$\n",
        "    - W: input height/width\n",
        "    - K: **filter size = 2**\n",
        "    - S: **stride size = filter size**, PyTorch defaults the stride to kernel filter size\n",
        "        - If using PyTorch default stride, this will result in the formula $ O = \\frac {W}{K}$\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkz3poJyn36V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "     \n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "        \n",
        "        # Max pool 1\n",
        "        out = self.maxpool1(out)\n",
        "        \n",
        "        # Convolution 2 \n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "        \n",
        "        # Max pool 2 \n",
        "        out = self.maxpool2(out)\n",
        "        \n",
        "        # Resize\n",
        "        # Original size: (100, 32, 7, 7)\n",
        "        # out.size(0): 100\n",
        "        # New out size: (100, 32*7*7)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb6FWnPXn36d",
        "colab_type": "text"
      },
      "source": [
        "### Instantiate Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTpkQFokn36d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CNNModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kthxTnFMn36k",
        "colab_type": "text"
      },
      "source": [
        "### Instantiate Loss Class\n",
        "- Convolutional Neural Network: **Cross Entropy Loss**\n",
        "    - _Feedforward Neural Network_: **Cross Entropy Loss**\n",
        "    - _Logistic Regression_: **Cross Entropy Loss**\n",
        "    - _Linear Regression_: **MSE**\n",
        "    \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh7zgtJcn36m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykeKMQkzn36s",
        "colab_type": "text"
      },
      "source": [
        "### Instantiate Optimizer Class\n",
        "- Simplified equation\n",
        "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
        "        - $\\theta$: parameters (our variables)\n",
        "        - $\\eta$: learning rate (how fast we want to learn)\n",
        "        - $\\nabla_\\theta$: parameters' gradients\n",
        "- Even simplier equation\n",
        "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
        "    - **At every iteration, we update our model's parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UcdtvUPn36t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQybwnjn360",
        "colab_type": "text"
      },
      "source": [
        "### Parameters In-Depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuoEJFa9n362",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "49e07612-4e2d-4732-c06a-53d4813f8581"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNNModel(\n",
            "  (cnn1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (relu1): ReLU()\n",
            "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (relu2): ReLU()\n",
            "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j23vB7Jln367",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b30528f9-cdc8-4b15-d692-049a71dd3ad1"
      },
      "source": [
        "print(model.parameters())\n",
        "\n",
        "print(len(list(model.parameters())))\n",
        "\n",
        "# Convolution 1: 16 Kernels\n",
        "print(list(model.parameters())[0].size())\n",
        "\n",
        "# Convolution 1 Bias: 16 Kernels\n",
        "print(list(model.parameters())[1].size())\n",
        "\n",
        "# Convolution 2: 32 Kernels with depth = 16\n",
        "print(list(model.parameters())[2].size())\n",
        "\n",
        "# Convolution 2 Bias: 32 Kernels with depth = 16\n",
        "print(list(model.parameters())[3].size())\n",
        "\n",
        "# Fully Connected Layer 1\n",
        "print(list(model.parameters())[4].size())\n",
        "\n",
        "# Fully Connected Layer Bias\n",
        "print(list(model.parameters())[5].size())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object Module.parameters at 0x7f5514270b48>\n",
            "6\n",
            "torch.Size([16, 1, 5, 5])\n",
            "torch.Size([16])\n",
            "torch.Size([32, 16, 5, 5])\n",
            "torch.Size([32])\n",
            "torch.Size([10, 1568])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_CyTOC-n37C",
        "colab_type": "text"
      },
      "source": [
        "### Train Model\n",
        "- Process \n",
        "    1. **Convert inputs/labels to variables**\n",
        "        - CNN Input: (1, 28, 28) \n",
        "        - Feedforward NN Input: (1, 28*28)\n",
        "    2. Clear gradient buffets\n",
        "    3. Get output given inputs \n",
        "    4. Get loss\n",
        "    5. Get gradients w.r.t. parameters\n",
        "    6. Update parameters using gradients\n",
        "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
        "    7. Repeat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Muh2gcRn37C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "408600f7-417e-4d20-f8f2-84258f786a69"
      },
      "source": [
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images\n",
        "        images = images.requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images\n",
        "                images = images.requires_grad_()\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.34400519728660583. Accuracy: 90\n",
            "Iteration: 1000. Loss: 0.3310098350048065. Accuracy: 93\n",
            "Iteration: 1500. Loss: 0.18019235134124756. Accuracy: 94\n",
            "Iteration: 2000. Loss: 0.09299946576356888. Accuracy: 95\n",
            "Iteration: 2500. Loss: 0.21474985778331757. Accuracy: 96\n",
            "Iteration: 3000. Loss: 0.1488540917634964. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9P-Ha7_n37I",
        "colab_type": "text"
      },
      "source": [
        "### Model B: \n",
        "- 2 Convolutional Layers\n",
        "    - Same Padding (same output size)\n",
        "- 2 **Average Pooling** Layers\n",
        "- 1 Fully Connected Layer\n",
        "\n",
        "\n",
        "### Steps\n",
        "- Step 1: Load Dataset\n",
        "- Step 2: Make Dataset Iterable\n",
        "- Step 3: Create Model Class\n",
        "- Step 4: Instantiate Model Class\n",
        "- Step 5: Instantiate Loss Class\n",
        "- Step 6: Instantiate Optimizer Class\n",
        "- Step 7: Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af-TKBUxn37J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3c8b3df3-c094-45fa-ba97-386bc2ba3cc3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # Average pool 1\n",
        "        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n",
        "     \n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        # Average pool 2\n",
        "        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n",
        "        \n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "        \n",
        "        # Average pool 1\n",
        "        out = self.avgpool1(out)\n",
        "        \n",
        "        # Convolution 2 \n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "        \n",
        "        # Max pool 2 \n",
        "        out = self.avgpool2(out)\n",
        "        \n",
        "        # Resize\n",
        "        # Original size: (100, 32, 7, 7)\n",
        "        # out.size(0): 100\n",
        "        # New out size: (100, 32*7*7)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "\n",
        "model = CNNModel()\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.requires_grad_()\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.42317673563957214. Accuracy: 86\n",
            "Iteration: 1000. Loss: 0.37601861357688904. Accuracy: 89\n",
            "Iteration: 1500. Loss: 0.28816813230514526. Accuracy: 90\n",
            "Iteration: 2000. Loss: 0.3652282655239105. Accuracy: 91\n",
            "Iteration: 2500. Loss: 0.414783775806427. Accuracy: 92\n",
            "Iteration: 3000. Loss: 0.17860358953475952. Accuracy: 93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMzrttbQn37V",
        "colab_type": "text"
      },
      "source": [
        "### Average Pooling Test Accuracy < Max Pooling Test Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFVmEPRin37W",
        "colab_type": "text"
      },
      "source": [
        "### Model C: \n",
        "- 2 Convolutional Layers\n",
        "    - **Valid Padding** (smaller output size)\n",
        "- 2 **Max Pooling** Layers\n",
        "- 1 Fully Connected Layer\n",
        "<img src=\"./images/cnn10-5.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
        "<img src=\"./images/cnn10-6n.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
        "\n",
        "### Steps\n",
        "- Step 1: Load Dataset\n",
        "- Step 2: Make Dataset Iterable\n",
        "- Step 3: Create Model Class\n",
        "- Step 4: Instantiate Model Class\n",
        "- Step 5: Instantiate Loss Class\n",
        "- Step 6: Instantiate Optimizer Class\n",
        "- Step 7: Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf-HWc8Wn37X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "43d83b08-b2e1-447c-b920-696c0f61e7de"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "     \n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "        \n",
        "        # Max pool 1\n",
        "        out = self.maxpool1(out)\n",
        "        \n",
        "        # Convolution 2 \n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "        \n",
        "        # Max pool 2 \n",
        "        out = self.maxpool2(out)\n",
        "        \n",
        "        # Resize\n",
        "        # Original size: (100, 32, 7, 7)\n",
        "        # out.size(0): 100\n",
        "        # New out size: (100, 32*7*7)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "\n",
        "model = CNNModel()\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.requires_grad_()\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.44368550181388855. Accuracy: 87\n",
            "Iteration: 1000. Loss: 0.4115293622016907. Accuracy: 90\n",
            "Iteration: 1500. Loss: 0.23302818834781647. Accuracy: 93\n",
            "Iteration: 2000. Loss: 0.20450137555599213. Accuracy: 95\n",
            "Iteration: 2500. Loss: 0.0990058109164238. Accuracy: 95\n",
            "Iteration: 3000. Loss: 0.12351467460393906. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXDihjphn37c",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Deep Learning\n",
        "- 3 ways to expand a convolutional neural network\n",
        "    - More convolutional layers \n",
        "    - Less aggressive downsampling\n",
        "        - Smaller kernel size for pooling (gradually downsampling)\n",
        "    - More fully connected layers \n",
        "- Cons\n",
        "    - Need a larger dataset\n",
        "        - Curse of dimensionality\n",
        "    - Does not necessarily mean higher accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os9V4wXan37d",
        "colab_type": "text"
      },
      "source": [
        "### Building a Convolutional Neural Network with PyTorch (GPU)\n",
        "### Model A\n",
        "\n",
        "Two things must be on GPU\n",
        "- `model`\n",
        "- `variables`\n",
        "\n",
        "### Steps\n",
        "- Step 1: Load Dataset\n",
        "- Step 2: Make Dataset Iterable\n",
        "- Step 3: Create Model Class\n",
        "- **Step 4: Instantiate Model Class**\n",
        "- Step 5: Instantiate Loss Class\n",
        "- Step 6: Instantiate Optimizer Class\n",
        "- **Step 7: Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQuemD2cn37e",
        "colab_type": "code",
        "colab": {},
        "outputId": "426124c1-f0ac-4901-ab7a-08665ceeefe5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "     \n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "        \n",
        "        # Max pool 1\n",
        "        out = self.maxpool1(out)\n",
        "        \n",
        "        # Convolution 2 \n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "        \n",
        "        # Max pool 2 \n",
        "        out = self.maxpool2(out)\n",
        "        \n",
        "        # Resize\n",
        "        # Original size: (100, 32, 7, 7)\n",
        "        # out.size(0): 100\n",
        "        # New out size: (100, 32*7*7)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "\n",
        "model = CNNModel()\n",
        "\n",
        "#######################\n",
        "#  USE GPU FOR MODEL  #\n",
        "#######################\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        \n",
        "        #######################\n",
        "        #  USE GPU FOR MODEL  #\n",
        "        #######################\n",
        "        images = images.requires_grad_().to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                #######################\n",
        "                #  USE GPU FOR MODEL  #\n",
        "                #######################\n",
        "                images = images.requires_grad_().to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                #######################\n",
        "                #  USE GPU FOR MODEL  #\n",
        "                #######################\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.2899038791656494. Accuracy: 89\n",
            "Iteration: 1000. Loss: 0.35760563611984253. Accuracy: 92\n",
            "Iteration: 1500. Loss: 0.24162068963050842. Accuracy: 94\n",
            "Iteration: 2000. Loss: 0.2910376489162445. Accuracy: 95\n",
            "Iteration: 2500. Loss: 0.09718359261751175. Accuracy: 96\n",
            "Iteration: 3000. Loss: 0.1140664592385292. Accuracy: 96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEPNbw5Mn37i",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5m78HChn37k",
        "colab_type": "text"
      },
      "source": [
        "- Transition from **Feedforward Neural Network**\n",
        "    - Addition of **Convolutional** & **Pooling** Layers before Linear Layers\n",
        "- One **Convolutional** Layer Basics\n",
        "- One **Pooling** Layer Basics\n",
        "    - Max pooling\n",
        "    - Average pooling\n",
        "- **Padding **\n",
        "- **Output Dimension** Calculations and Examples\n",
        "    -  $ O = \\frac {W - K + 2P}{S} + 1$\n",
        "- Convolutional Neural Networks\n",
        "    - **Model A**: 2 Conv + 2 Max pool + 1 FC\n",
        "        - Same Padding\n",
        "    - **Model B**: 2 Conv + 2 Average pool + 1 FC\n",
        "        - Same Padding\n",
        "    - **Model C**: 2 Conv + 2 Max pool + 1 FC\n",
        "        - Valid Padding\n",
        "- Model Variation in **Code**\n",
        "    - Modifying only step 3\n",
        "- Ways to Expand Modelâ€™s **Capacity**\n",
        "    - More convolutions\n",
        "    - Gradual pooling\n",
        "    - More fully connected layers\n",
        "- **GPU** Code\n",
        "    - 2 things on GPU\n",
        "        - **model**\n",
        "        - **variable**\n",
        "    - Modifying only **Step 4 & Step 7**\n",
        "- **7 Step** Model Building Recap\n",
        "    - Step 1: Load Dataset\n",
        "    - Step 2: Make Dataset Iterable\n",
        "    - Step 3: Create Model Class\n",
        "    - Step 4: Instantiate Model Class\n",
        "    - Step 5: Instantiate Loss Class\n",
        "    - Step 6: Instantiate Optimizer Class\n",
        "    - Step 7: Train Model"
      ]
    }
  ]
}