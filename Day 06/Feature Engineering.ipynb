{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oxw5y5D_2oK",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering In Machine Learning\n",
        "\n",
        "### Good System\n",
        "- Features to represent data\n",
        "- Speed and efficiency\n",
        "- Small \n",
        "\n",
        "### Preprocessing\n",
        "- Create good features \n",
        "- Synthetic features\n",
        "\n",
        "  Using synthetic data, generating more images from existing images.\n",
        "  If you have more datapoints with certain features they will be learned better and incorporated into the model.\n",
        "\n",
        "### Good Features \n",
        "\n",
        "#### 1. Related to objective\n",
        "\n",
        "Data not related will act as noise.\n",
        "\n",
        "#### 2. Known at the time of predictation\n",
        "\n",
        "  Common example is sales data, where they need month old data, but only cyrrent data is available.\n",
        "\n",
        "  **Stale data versus Real time data**\n",
        "\n",
        "#### 3. Numeric with meaningful magnitude\n",
        "\n",
        "  NNs are weighing and adding machines and require numeric input.\n",
        "\n",
        "  When classinfication **One-Hot Key encoding** is used to give meaning to the magnitude.\n",
        "\n",
        "  When a new key is obtain give it a speacial value, all zeroes for example.\n",
        "\n",
        "  But these encoding can get really large, then a possible solutions are:\n",
        "  - Learnable encodings\n",
        "  - Hash buckets for keys \n",
        "\n",
        "  Here the categories are grouped and one-hot encoding is done on the groups. Or a hash function with groups again.\n",
        "\n",
        "#### 4. Enough valid examples\n",
        "\n",
        "  For example the feature may be related like gender while predicting height but if the dataset contains only men then its useless.\n",
        "\n",
        "#### 5. Human insight \n",
        "\n",
        "  This means that when the features supplied to a human a decision is possible.\n",
        "\n",
        "***Note:***\n",
        "Human insight is not really needed in deep learning.\n",
        "Deep learning also requires a large amount of data."
      ]
    }
  ]
}